/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
cpu
torch.Size([16, 100])
Traceback (most recent call last):
  File "/home/ivlabs/repos/tfs/tranlslation-deq-main/train.py", line 88, in <module>
    output = model(output.long(),trg)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/repos/tfs/tranlslation-deq-main/model.py", line 38, in forward
    out = self.dec(tgt=tgt, memory=enc_output, memory_mask=mask,tgt_mask = tgt_mask,tgt_is_causal=True, memory_is_causal=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 465, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 856, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 874, in _mha_block
    x = self.multihead_attn(x, mem, mem,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivlabs/anaconda3/envs/nlp2/lib/python3.12/site-packages/torch/nn/functional.py", line 5382, in multi_head_attention_forward
    k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: shape '[16, 64, 192]' is invalid for input of size 1228800